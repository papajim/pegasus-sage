{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22660fa9-8b90-4198-b2dc-2794cefe40b3",
   "metadata": {},
   "source": [
    "# CROCUS Processing Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb49bf-fb0f-466c-993b-8ed3a99748d9",
   "metadata": {},
   "source": [
    "## Imports Required for Workflow Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694e554f-4b24-49a1-869d-e9c2c9aa60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from Pegasus.api import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5f38c-1b80-4182-a671-0322d24a3ad4",
   "metadata": {},
   "source": [
    "## Class That Handles Workflow Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ffc511-f698-404c-ab78-2975ab6b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrocusWorkflow():\n",
    "    wf = None\n",
    "    sc = None\n",
    "    tc = None\n",
    "    rc = None\n",
    "    props = None\n",
    "\n",
    "    dagfile = None\n",
    "    wf_dir = None\n",
    "    shared_scratch_dir = None\n",
    "    local_storage_dir = None\n",
    "    wf_name = \"crocus\"\n",
    "\n",
    "    # --- Init ---------------------------------------------------------------------\n",
    "    def __init__(self, aqt_end_date, wxt_end_date, aqt_lag=1, wxt_lag=1, dagfile=\"workflow.yml\"):\n",
    "        self.dagfile = dagfile\n",
    "\n",
    "        self.wf_dir = str(Path.cwd())\n",
    "        self.shared_scratch_dir = os.path.join(self.wf_dir, \"scratch\")\n",
    "        self.local_storage_dir = os.path.join(self.wf_dir, \"output\")\n",
    "\n",
    "        self.aqt_end_date = aqt_end_date\n",
    "        self.aqt_lag = aqt_lag\n",
    "        self.wxt_end_date = wxt_end_date\n",
    "        self.wxt_lag = wxt_lag\n",
    "\n",
    "\n",
    "    # --- Write files in directory -------------------------------------------------\n",
    "    def write(self):\n",
    "        self.sc.write()\n",
    "        self.props.write()\n",
    "        self.rc.write()\n",
    "        self.tc.write()\n",
    "        self.wf.write()\n",
    "\n",
    "\n",
    "    # --- Configuration (Pegasus Properties) ---------------------------------------\n",
    "    def create_pegasus_properties(self):\n",
    "        self.props = Properties()\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Site Catalog -------------------------------------------------------------\n",
    "    def create_sites_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.sc = SiteCatalog()\n",
    "\n",
    "        local = (Site(\"local\")\n",
    "                    .add_directories(\n",
    "                        Directory(Directory.SHARED_SCRATCH, self.shared_scratch_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.shared_scratch_dir, Operation.ALL)),\n",
    "                        Directory(Directory.LOCAL_STORAGE, self.local_storage_dir)\n",
    "                            .add_file_servers(FileServer(\"file://\" + self.local_storage_dir, Operation.ALL))\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        exec_site = (Site(exec_site_name)\n",
    "                        .add_condor_profile(universe=\"vanilla\")\n",
    "                        .add_pegasus_profile(\n",
    "                            style=\"condor\"\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "\n",
    "        self.sc.add_sites(local, exec_site)\n",
    "        return \n",
    "\n",
    "\n",
    "    # --- Transformation Catalog (Executables and Containers) ----------------------\n",
    "    def create_transformation_catalog(self, exec_site_name=\"condorpool\"):\n",
    "        self.tc = TransformationCatalog()\n",
    "\n",
    "        crocus_container = Container(\"crocus_container\",\n",
    "            container_type = Container.SINGULARITY,\n",
    "            image=\"docker://papajim/crocus:latest\",\n",
    "            image_site=\"docker_hub\"\n",
    "        )\n",
    "\n",
    "        # Add the crocus processing\n",
    "        aqt_ingest = Transformation(\"aqt_ingest\", site=exec_site_name, pfn=os.path.join(self.wf_dir, \"executables/aqt-ingest.py\"), is_stageable=True, container=crocus_container)\n",
    "        wxt_ingest = Transformation(\"wxt_ingest\", site=exec_site_name, pfn=os.path.join(self.wf_dir, \"executables/wxt-ingest.py\"), is_stageable=True, container=crocus_container)\n",
    "\n",
    "\n",
    "        self.tc.add_containers(crocus_container)\n",
    "        self.tc.add_transformations(aqt_ingest, wxt_ingest)\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Replica Catalog ----------------------------------------------------------\n",
    "    def create_replica_catalog(self):\n",
    "        self.rc = ReplicaCatalog()\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Create Workflow ----------------------------------------------------------\n",
    "    def create_workflow(self):\n",
    "        self.wf = Workflow(self.wf_name, infer_dependencies=True)\n",
    "        \n",
    "        #create aqt jobs\n",
    "        aqt_start_date = self.aqt_end_date - timedelta(days=self.aqt_lag)\n",
    "        for i in range(self.aqt_lag):\n",
    "            curr_aqt_date = aqt_start_date + timedelta(days=i)\n",
    "            date_time_str = curr_aqt_date.strftime(\"%Y-%m-%d\")\n",
    "            output_file = curr_aqt_date.strftime('crocus-neiu-aqt-a1-%Y%m%d-%H%M%S.nc'), \n",
    "            aqt_ingest_job = (\n",
    "                Job(\"aqt_ingest\", _id=f\"crocus-neiu-aqt-{date_time_str}\", node_label=f\"crocus-neiu-aqt-{date_time_str}\")\n",
    "                    .add_args(\"--date\", curr_aqt_date.strftime(\"%Y-%m-%d\"))\n",
    "                    .add_outputs(curr_aqt_date.strftime('crocus-neiu-aqt-a1-%Y%m%d-%H%M%S.nc'), register_replica=True, stage_out=True)\n",
    "            )\n",
    "        \n",
    "            self.wf.add_jobs(aqt_ingest_job)\n",
    "\n",
    "\n",
    "\n",
    "        #create wxt jobs\n",
    "        wxt_start_date = self.wxt_end_date - timedelta(days=self.wxt_lag)\n",
    "        for i in range(self.wxt_lag):\n",
    "            curr_wxt_date = wxt_start_date + timedelta(days=i)\n",
    "            date_time_str = curr_wxt_date.strftime(\"%Y-%m-%d\")\n",
    "            output_file = curr_wxt_date.strftime('crocus-neiu-wxt-a1-%Y%m%d-%H%M%S.nc')\n",
    "            wxt_ingest_job = (Job(\"wxt_ingest\", _id=f\"crocus-neiu-wxt-{date_time_str}\", node_label=f\"crocus-neiu-wxt-{date_time_str}\")\n",
    "                    .add_args(\"--date\", date_time_str)\n",
    "                    .add_outputs(output_file, register_replica=True, stage_out=True)\n",
    "            )\n",
    "        \n",
    "            self.wf.add_jobs(wxt_ingest_job)\n",
    "\n",
    "    # --- Plan and Submit the Workflow ---------------------------------------------\n",
    "    def plan_and_submit(self):\n",
    "        try:\n",
    "            self.wf.plan(\n",
    "                submit=True,\n",
    "                dir=\"./submit\",\n",
    "                output_sites=[\"local\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # --- Get status of the workflow -----------------------------------------------\n",
    "    def status(self):\n",
    "        try:\n",
    "            self.wf.status(long=True)\n",
    "        except PegasusClientError as e:\n",
    "            print(e)\n",
    "            \n",
    "    # --- Get statistics of the workflow -------------------------------------------\n",
    "    def statistics(self):\n",
    "        try:\n",
    "            self.wf.statistics()\n",
    "        except PegasusClientError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28233141-64c3-47cb-816f-0fbece195aca",
   "metadata": {},
   "source": [
    "## Generate The Workflow\n",
    "The CrocusWorkflow class accepts an AQT end date, an AQT lag, a WXT end date and a WXT lag.\n",
    "The end date is not included in the pulled data, insted it is used as a reference for the lag to pull data for LAG previous days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d2418d-38da-4945-8359-21c8e9bc14b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating execution sites...\n",
      "Creating workflow properties...\n",
      "Creating transformation catalog...\n",
      "Creating replica catalog...\n",
      "Creating crocus workflow dag...\n",
      "defaultdict(<class 'collections.OrderedDict'>, {})\n",
      "defaultdict(<class 'collections.OrderedDict'>, {})\n"
     ]
    }
   ],
   "source": [
    "workflow = CrocusWorkflow(\n",
    "    aqt_end_date = datetime.strptime(\"2024-03-15\", \"%Y-%m-%d\"),\n",
    "    wxt_end_date = datetime.strptime(\"2024-03-15\", \"%Y-%m-%d\"),\n",
    "    aqt_lag = 3,\n",
    "    wxt_lag = 3\n",
    ")\n",
    "\n",
    "print(\"Creating execution sites...\")\n",
    "workflow.create_sites_catalog()\n",
    "\n",
    "print(\"Creating workflow properties...\")\n",
    "workflow.create_pegasus_properties()\n",
    "\n",
    "print(\"Creating transformation catalog...\")\n",
    "workflow.create_transformation_catalog()\n",
    "\n",
    "print(\"Creating replica catalog...\")\n",
    "workflow.create_replica_catalog()\n",
    "\n",
    "print(\"Creating crocus workflow dag...\")\n",
    "workflow.create_workflow()\n",
    "\n",
    "workflow.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e5d33-abd4-4110-8f85-13fb1bf248c8",
   "metadata": {},
   "source": [
    "## Plan and Submit the Workflow\n",
    "We will now plan and submit the workflow for execution. By default we are running jobs on site condorpool i.e the selected ACCESS resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6fa4b7c-e641-4889-819d-2c1beb76abf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024.03.19 14:59:04.784 PDT:\n",
      "2024.03.19 14:59:04.790 PDT:   -----------------------------------------------------------------------\n",
      "2024.03.19 14:59:04.795 PDT:   File for submitting this DAG to HTCondor           : crocus-0.dag.condor.sub\n",
      "2024.03.19 14:59:04.800 PDT:   Log of DAGMan debugging messages                 : crocus-0.dag.dagman.out\n",
      "2024.03.19 14:59:04.806 PDT:   Log of HTCondor library output                     : crocus-0.dag.lib.out\n",
      "2024.03.19 14:59:04.812 PDT:   Log of HTCondor library error messages             : crocus-0.dag.lib.err\n",
      "2024.03.19 14:59:04.817 PDT:   Log of the life of condor_dagman itself          : crocus-0.dag.dagman.log\n",
      "2024.03.19 14:59:04.823 PDT:\n",
      "2024.03.19 14:59:04.828 PDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with:\n",
      "2024.03.19 14:59:04.839 PDT:   -----------------------------------------------------------------------\n",
      "2024.03.19 14:59:05.353 PDT:   Database version: '5.0.8dev' (sqlite:////home/georgpap/.pegasus/workflow.db)\n",
      "2024.03.19 14:59:06.194 PDT:   Pegasus database was successfully created.\n",
      "2024.03.19 14:59:06.199 PDT:   Database version: '5.0.8dev' (sqlite:////home/georgpap/GitHub/papajim/pegasus-sage/crocus-processing/submit/georgpap/pegasus/crocus/run0001/crocus-0.replicas.db)\n",
      "2024.03.19 14:59:06.223 PDT:   Output replica catalog set to jdbc:sqlite:/home/georgpap/GitHub/papajim/pegasus-sage/crocus-processing/submit/georgpap/pegasus/crocus/run0001/crocus-0.replicas.db\n",
      "[WARNING]  Submitting to condor crocus-0.dag.condor.sub\n",
      "2024.03.19 14:59:06.613 PDT:   Time taken to execute is 2.495 seconds\n"
     ]
    }
   ],
   "source": [
    "workflow.plan_and_submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438826b-1e7b-438c-993f-edc6f02e86f5",
   "metadata": {},
   "source": [
    "After the workflow has been successfully planned and submitted, you can use the Python Workflow object in order to monitor the status of the workflow. It shows in detail the counts of jobs of each status and also the whether the job is idle or running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b688f751-a55b-43ce-9663-3763d614a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "##################\n",
      "# pegasus-status #\n",
      "##################\n",
      "STAT  IN_STATE  JOB\n",
      "Run      02:37  crocus-0 ( /home/georgpap/GitHub/papajim/pegasus-sage/crocus-processing/submit/georgpap/pegasus/crocus/run0001 )\n",
      "Run      01:52   ┣━wxt_ingest_crocus-neiu-wxt-2024-03-13\n",
      "Run      01:52   ┣━wxt_ingest_crocus-neiu-wxt-2024-03-14\n",
      "Run      01:52   ┗━wxt_ingest_crocus-neiu-wxt-2024-03-12\n",
      "Summary: 4 Condor jobs total (R:4)\n",
      "\n",
      "UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME\n",
      "6     0     0     3     0     5     0  35.7 Running *crocus-0.dag\n",
      "Summary: 1 DAG total (Running:1)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "workflow.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee2fba8-a167-4e37-a3da-4279ee18a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;32m#########################\u001b[0m] 100.0% ..Success (\u001b[1;34mUnready: 0\u001b[0m, \u001b[1;32mCompleted: 14\u001b[0m, \u001b[1;33mQueued: 0\u001b[0m, \u001b[1;36mRunning: 0\u001b[0m, \u001b[1;31mFailed: 0\u001b[0m)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Pegasus.api.workflow.Workflow at 0x7fc2b028dc10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.wf.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00cb091-daf0-4a9b-927f-86571ae24ce8",
   "metadata": {},
   "source": [
    "##  Launch Pilots Jobs on ACCESS resources\n",
    "At this point you should have some idle jobs in the queue. They are idle because there are no resources yet to execute on. Resources can be brought in with the HTCondor Annex tool, by sending pilot jobs (also called glideins) to the ACCESS resource providers. These pilots have the following properties:\n",
    "\n",
    "A pilot can run multiple user jobs - it stays active until no more user jobs are available or until end of life has been reached, whichever comes first.\n",
    "\n",
    "A pilot is partitionable - job slots will dynamically be created based on the resource requirements in the user jobs. This means you can fit multiple user jobs on a compute node at the same time.\n",
    "\n",
    "A pilot will only run jobs for the user who started it.\n",
    "\n",
    "The process of starting pilots is described in the [ACCESS Pegasus Documentation](https://xsedetoaccess.ccs.uky.edu/confluence/redirect/ACCESS+Pegasus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5846bc-d014-4dac-b53d-fd09b30dd56d",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next.\n",
    "If the workflow failed you can use `workflow.analyze()` do get help finding out what went wrong.\n",
    "If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787fc73a-0f0e-4e35-b05e-a60ae28b4dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "######################\n",
      "# pegasus-statistics #\n",
      "######################\n",
      "Database version: '5.0.8dev' (sqlite:////home/georgpap/GitHub/papajim/pegasus-sage/crocus-processing/submit/georgpap/pegasus/crocus/run0001/crocus-0.stampede.db)\n",
      "\n",
      "#\n",
      "# Pegasus Workflow Management System - http://pegasus.isi.edu\n",
      "#\n",
      "# Workflow summary:\n",
      "#   Summary of the workflow execution. It shows total\n",
      "#   tasks/jobs/sub workflows run, how many succeeded/failed etc.\n",
      "#   In case of hierarchical workflow the calculation shows the\n",
      "#   statistics across all the sub workflows.It shows the following\n",
      "#   statistics about tasks, jobs and sub workflows.\n",
      "#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.\n",
      "#     * Failed - total count of failed tasks/jobs/sub workflows.\n",
      "#     * Incomplete - total count of tasks/jobs/sub workflows that are\n",
      "#       not in succeeded or failed state. This includes all the jobs\n",
      "#       that are not submitted, submitted but not completed etc. This\n",
      "#       is calculated as  difference between 'total' count and sum of\n",
      "#       'succeeded' and 'failed' count.\n",
      "#     * Total - total count of tasks/jobs/sub workflows.\n",
      "#     * Retries - total retry count of tasks/jobs/sub workflows.\n",
      "#     * Total+Retries - total count of tasks/jobs/sub workflows executed\n",
      "#       during workflow run. This is the cumulative of retries,\n",
      "#       succeeded and failed count.\n",
      "# Workflow wall time:\n",
      "#   The wall time from the start of the workflow execution to the end as\n",
      "#   reported by the DAGMAN.In case of rescue dag the value is the\n",
      "#   cumulative of all retries.\n",
      "# Cumulative job wall time:\n",
      "#   The sum of the wall time of all jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job wall time as seen from submit side:\n",
      "#   The sum of the wall time of all jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "# Cumulative job badput wall time:\n",
      "#   The sum of the wall time of all failed jobs as reported by kickstart.\n",
      "#   In case of job retries the value is the cumulative of all retries.\n",
      "#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),\n",
      "#   the wall time value includes jobs from the sub workflows as well.\n",
      "# Cumulative job badput wall time as seen from submit side:\n",
      "#   The sum of the wall time of all failed jobs as reported by DAGMan.\n",
      "#   This is similar to the regular cumulative job badput wall time, but includes\n",
      "#   job management overhead and delays. In case of job retries the value\n",
      "#   is the cumulative of all retries. For workflows having sub workflow\n",
      "#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs\n",
      "#   from the sub workflows as well.\n",
      "------------------------------------------------------------------------------\n",
      "Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries\n",
      "Tasks          6         0       0           6         0         6\n",
      "Jobs           14        0       0           14        0         14\n",
      "Sub-Workflows  0         0       0           0         0         0\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Workflow wall time                                       : 12 mins, 24 secs\n",
      "Cumulative job wall time                                 : 31 mins, 50 secs\n",
      "Cumulative job wall time as seen from submit side        : 32 mins, 8 secs\n",
      "Cumulative job badput wall time                          : 0.0 secs\n",
      "Cumulative job badput wall time as seen from submit side : 0.0 secs\n",
      "\n",
      "# Integrity Metrics\n",
      "# Number of files for which checksums were compared/computed along with total time spent doing it.\n",
      "24 files checksums compared with total duration of 8.24 secs\n",
      "9 files checksums generated with total duration of 0.56 secs\n",
      "\n",
      "# Integrity Errors\n",
      "# Total:\n",
      "#       Total number of integrity errors encountered across all job executions(including retries) of a workflow.\n",
      "# Failures:\n",
      "#       Number of failed jobs where the last job instance had integrity errors.\n",
      "Failures: 0 job failures had integrity errors\n",
      "\n",
      "Summary                       : /home/georgpap/GitHub/papajim/pegasus-sage/crocus-processing/submit/georgpap/pegasus/crocus/run0001/statistics/summary.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workflow.statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2b9a1-8539-43db-a0bf-698da7c4dc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
